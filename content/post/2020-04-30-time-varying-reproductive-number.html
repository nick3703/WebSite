---
title: Time Varying Reproductive Number
author: Clark
date: '2020-04-30'
slug: time-varying-reproductive-number
categories: []
tags: []
description: ''
---



<p>Note that this analysis draws heavily from Nishiura and Chowell, 2009.</p>
<div id="shortcomings-of-sir-framework" class="section level2">
<h2>Shortcomings of SIR Framework</h2>
<p>To avoid confusion we follow Nishiura and Chowell and refer to <span class="math inline">\(S_t\)</span> as the number of suceptible individuals at time <span class="math inline">\(t\)</span>, <span class="math inline">\(I_t\)</span> as the number of infected individuals at time <span class="math inline">\(t\)</span> and <span class="math inline">\(U_t\)</span> as the number of individuals removed from the population (either no longer suceptible or deceased) at time <span class="math inline">\(t\)</span>.</p>
<p>The general SIR framework assumes that at time <span class="math inline">\(t\)</span> the population can be segmented into these three categories and is given by a system of differential equations:</p>
<p><span class="math display">\[\begin{align*}
 \frac{dS}{dt}&amp;= -\frac{\beta I_t S_t}{N}\\
 \frac{dI}{dT}&amp;= \frac{\beta I_t S_t}{N}-\gamma I_t\\
 \frac{dU}{dt}&amp;= \gamma I_t
\end{align*}\]</span></p>
<p>Here we play a bit fast and loose with the models as we could further could compartmentalize the population into an SEIR model, or a model with a deceased state, but the general framework remains the same.</p>
<p>Here we focus on the parameter <span class="math inline">\(\beta\)</span>, which has the general meaning of average number of contacts a suceptible person has. <span class="math inline">\(\beta\)</span> can be further decomposed into, <span class="math inline">\(\beta = R_0 \gamma\)</span> where <span class="math inline">\(\gamma\)</span> is the rate that Infected people are removed from the population. For the purpose of our analysis we fix <span class="math inline">\(\gamma\)</span> and concentrate on <span class="math inline">\(R_0\)</span>, which is often called the basic reproduction number of the virus. <span class="math inline">\(R_0\)</span> has received much attention and can be thought of as the number of new infections that can be caused by an individual with the illness. In simple terms, if this number is bigger than 1 the number of people in the population will continue to grow until herd immunity starts to kick in and if this number is smaller than 1 the virus will shrink.</p>
<p>The issue is, SIR and related models are extremely sensitive to <span class="math inline">\(R_0\)</span>. For instance, below we show the difference in New York City in the number of infected people ranging <span class="math inline">\(R_0\)</span> from 3 (blue line) to 2 (green line)</p>
<p><img src="/post/2020-04-30-time-varying-reproductive-number_files/figure-html/unnamed-chunk-1-1.png" width="672" /></p>
<p>Here we see that a difference in 1 <span class="math inline">\(R_0\)</span> corresponds to a difference in over half a million infected as well as a difference in a peak day of over a month.</p>
<p>While a useful projeciton framework for thinking about the baseline characteristics of the virus, this isn’t helpful in forecasting what actually happens.</p>
<p>To see what actually happens we need to see what the effective <span class="math inline">\(R_t\)</span> of the virus is. Going back to the physical interpretation of <span class="math inline">\(\beta\)</span>, when society does things to impact the amount of interactions between individuals, we can, and should, expect <span class="math inline">\(\beta\)</span> to decrease, or in other words to expect <span class="math inline">\(R_t\)</span> to decrease.</p>
<p>In order to do this we need a way to estimate <span class="math inline">\(R_t\)</span>. We refer to <span class="math inline">\(\hat{R}_t\)</span> as our empirical estimate of <span class="math inline">\(R_t\)</span> and, following Nishiura and Chowell, 2009, we note this can be estimated (eq. 33) via:</p>
<p><span class="math display">\[\begin{align*}
\hat{R}_t &amp;= \frac{j_t}{\sum_{j=0}^n j_{t-j} w_j}
\end{align*}\]</span></p>
<p>Where here <span class="math inline">\(j_t\)</span> are the number of new cases observed on day <span class="math inline">\(t\)</span> and <span class="math inline">\(w\)</span> is the disretized generation time distribution. Note that the generation time distribution can be thought of as the expected time between the emergence of a secondary case after a primary case is observed.</p>
<p>For COVID-19, initial work done by Nishiura, Lintona, and Akhmetzhanov’s (2020) “Serial interval of novel coronavirus (COVID-19) infections” provides a more extensive analysis of the generation time distribution. In their analysis, “the median serial interval of the best-fit Weibull distribution model was estimated at with a mean and SD of 4.8 days and 2.3 days.”</p>
<p>Note that this a model assumption that captures the parameter assumptions in the orignal SIR framework. In otherwords, if we know the generation time distribution and all we are interested in is predicting new cases we don’t need <span class="math inline">\(\gamma\)</span>.</p>
<p>Applying the above estimates for <span class="math inline">\(\hat{R}_t\)</span> for Italy using the <code>library(R0)</code> we can run</p>
<pre class="r"><code>library(R0)
full.data&lt;-read_csv(&quot;https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv&quot;)
mGT &lt;- generation.time(&quot;weibull&quot;, c(4.3,2.3)) 
Italy&lt;-full.data%&gt;%filter(`Country/Region`==&quot;Italy&quot;,is.na(`Province/State`))%&gt;%
  dplyr::select(-`Province/State`,-`Country/Region`,-Lat,-Long)
vec&lt;-as.numeric(Italy)
daily&lt;-diff(vec)+1
daily[50:51]&lt;-daily[51]/2  #something weird with this day, assume
#lag in reporting combining two days
et&lt;-as.numeric(length(daily))
R.vals.Ital&lt;-est.R0.TD(daily,mGT,begin=48,end=et-1)
plot(R.vals.Ital)</code></pre>
<p><img src="/post/2020-04-30-time-varying-reproductive-number_files/figure-html/unnamed-chunk-2-1.png" width="672" /></p>
<p>Here we see that the <span class="math inline">\(R_t\)</span> drops <em>below</em> 1. This is significant because once <span class="math inline">\(R_t\)</span> drops below 1 the country has peaked regardless of what the SIR model or SEIR model initially projects.</p>
<p>This suggests that any SIR or SEIR model used relying on a fixed <span class="math inline">\(R_0\)</span> value will overpredict, potentially significantly.</p>
<p>For example, New York City dropped below 1 for a sustained amount of time on April 12th, 22 days after their lock down started.</p>
<p>In order to use <span class="math inline">\(\hat{R}_t\)</span> in predicting, we need to not only understand what <span class="math inline">\(\hat{R}_t\)</span> is, we also need to understand where <span class="math inline">\(\hat{R}_t\)</span> will be for future <span class="math inline">\(t\)</span> values. In order to do this we propose a parametric modeling technique.</p>
<p>The model we propose is:</p>
<p><span class="math display">\[\begin{align*}
\log(\nu_t)&amp;= \beta_0 + \beta_1 t\\
\hat{R}_t &amp; \sim Gamma(\nu_t,\phi \nu_t^2)
\end{align*}\]</span></p>
<p>Note above we parameterize the Gamma distribution using mean and variance parameterization. Essentially we use a GLM with a log link. Notable given <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> we can use <span class="math inline">\(E[\hat{R}_{t+j}]\)</span> to esimate what future <span class="math inline">\(\hat{R}_{t+j}\)</span> values will be.</p>
<p>Once we have the model framework, we propose the following to predict future values.</p>
<p>For each geographical area of interest</p>
<ul>
<li>Define <span class="math inline">\(t_n\)</span> as current time</li>
<li>Define <span class="math inline">\(I_t\)</span> as the new observed cases for <span class="math inline">\(t = 1, \cdots, t_n\)</span></li>
<li>Use the observed <span class="math inline">\(\hat{R}_t\)</span> values to estimate <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span></li>
<li>Use <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> to generate, for <span class="math inline">\(t = 1, \cdots,t_n,\cdots,T\)</span> model based <span class="math inline">\(R_t\)</span> values, we refer to as <span class="math inline">\(R^*_t\)</span></li>
<li>For <span class="math inline">\(t = 1, \cdots, t_n\)</span> Calculate <span class="math inline">\(R^*_t\times I_t \times p(w)\)</span> where <span class="math inline">\(p(w)\)</span> is the pmf for the discretized generational distribution, in our case a discretized weibull with mean 4.3 and SD 2.3</li>
</ul>
<p>Note that this creates a vector of when we would expect future cases to manifest. To give an example,if we observe 100 new cases and <span class="math inline">\(R^*_t=.8\)</span> we would see 80 new cases manifest</p>
<pre class="r"><code>library(mixdist)
library(extraDistr)
#Create generating distribution
mGT.params&lt;-weibullpar(4.8, 2.3, loc = 0)
alpha&lt;-mGT.params[2] # called shape in weibullpar, alpha in a discrete Weilbull
beta&lt;-mGT.params[1] # called scale in weibullpar, beta in a discrete Weibull.
q&lt;-exp(-as.numeric(alpha)^(-as.numeric(beta)))
#Discretize Weibull via the extraDistr package&#39;s ddweibull function
w&lt;- ddweibull(1:10, as.numeric(q), as.numeric(beta))

100*.8*w</code></pre>
<pre><code>##  [1]  6.509622 10.612999 13.027809 13.308465 11.738782  9.091790  6.235832
##  [8]  3.804578  2.069693  1.005034</code></pre>
<p>Here we see that 6.5 would be on day 1, 10.6 on day 2, 13.0 on day 3, etc.</p>
<p>However, in our algorithm we won’t use those projections until we are at <span class="math inline">\(t=t_n\)</span> For <span class="math inline">\(t&gt; t_n\)</span> we use the projected totals on that given day to generate future days.</p>
<p>In order to impliment this fully, though, we need to keep in mind that <span class="math inline">\(\hat{R}_t\)</span> is backwards calculated, meaning it doesn’t give what today is going to produce, but rather what produced today. In order to correct this, we propse skipping forward 4 days in the <span class="math inline">\(R^*_t\)</span> curve.</p>
<p>Full code to calculate this for each state is included below.</p>
<pre class="r"><code>library(tidyverse)
library(extraDistr)
library(mixdist) #used to recoved the Weibull parameters from the mean and sd
library(ciTools)
library(lubridate)


#This function accepts the cumulative confirmed COVID-19 cases by county by day.
#It then estimates R(t) for that county by day.
rt.func.v3_dusty&lt;-function(dat,mean.Weibull=4.8,sd.Weibull=2.3){
  r.vals&lt;-c()
  confirmed = dat$confirmed
  #get the Weibull parameters from mixdist&#39;s weibullpar function
  mGT.params&lt;-weibullpar(mean.Weibull, sd.Weibull, loc = 0)
  alpha&lt;-mGT.params[2] # called shape in weibullpar, alpha in a discrete Weilbull
  beta&lt;-mGT.params[1] # called scale in weibullpar, beta in a discrete Weibull
  #the extraDistr package uses an altrnative parameterization of the Weibull (q, beta) from
  #Nakagawa and Osaki (1975) where q = exp(-alpha^-beta), so...
  q&lt;-exp(-as.numeric(alpha)^(-as.numeric(beta)))
  #Discretize Weibull via the extraDistr package&#39;s ddweibull function
  w&lt;- ddweibull(0:1000, as.numeric(q), as.numeric(beta), log = FALSE)
  df&lt;-data.frame(counts=t(c(0,confirmed)))
  # df&lt;-data.frame(counts=t(dat[-(1:4)]))
  # first.case.index&lt;-min(which(df&gt;0))
  # total.cases&lt;-max(df)
  growth&lt;-diff(unlist(df))
  # growth&lt;-diff(df$counts)
  growth&lt;-pmax(growth, 0) # eliminate any erroneous downward shifts in the cumulative counts
  #Estimate R(t) from equation (33) of Nishiura and Chowell (2009)
  for(k in 2:length(growth)){
    r.vals[k-1]&lt;-growth[k]/(sum(growth[1:k]*rev(w[1:k])))
  }
  r_val_df &lt;- tibble(
    day = 1:length(r.vals), 
    r.vals = r.vals, 
    daily_new_cases = growth[-1])
  return(r_val_df)
}


jhu_all&lt;-read_csv(paste0(&quot;https://raw.githubusercontent.com/nick3703/Parametric-Modeling-for-Time-Varying-Reproducibility-Number/master/&quot;,lubridate::today(),&quot;_JHU.csv&quot;)

state.names&lt;-read_csv(&quot;https://raw.githubusercontent.com/nick3703/Parametric-Modeling-for-Time-Varying-Reproducibility-Number/master/StateNames.csv&quot;)


jhu_all&lt;-jhu_all%&gt;%mutate(date=as.Date(date,format=&quot;%m/%d/%Y&quot;))

jhu_state&lt;-jhu_all %&gt;% group_by(province_state,date)%&gt;%
  summarize(confirmed=sum(confirmed))


rt_est &lt;- 
  jhu_state %&gt;% ## cases confirmed by fips
  group_by(province_state) %&gt;% 
  nest() %&gt;%  ## one row per fips with nested df of confirmed cases
  mutate(ans = map(data, rt.func.v3_dusty)) %&gt;% ## applies the rt function to each dataframe
  unnest(ans) ## expands back to one dataframe


rt_est_wide&lt;-rt_est%&gt;%pivot_wider(id_cols=province_state,names_from = day,values_from = r.vals)

rt_est&lt;-rt_est%&gt;%
  filter(province_state %in%state.names$State)

list_states&lt;-unique(rt_est$province_state)

output&lt;-data.frame(state=NA,b0=NA,b1=NA,ucb=NA,curr.r=NA,
                   curr.emp.r=NA)

state&lt;-rt_est%&gt;%filter(province_state==&quot;New York&quot;) #Just getting rowsize here
cols.needed&lt;-length(state$daily_new_cases)

new.cases&lt;-matrix(0,nrow=length(unique(rt_est$province_state)),ncol=cols.needed)


for(j in 1:length(list_states)){
  state&lt;-rt_est%&gt;%filter(province_state==list_states[j])
  output[j,]$state=list_states[j]
  # Fit models with Gamma errors
  gam.glm &lt;- glm(r.vals+.01~day, family = Gamma(link = &quot;log&quot;),data=state)
  coefs&lt;-gam.glm$coefficients
  output[j,]$b0=coefs[1]
  output[j,]$b1=coefs[2]
  df = data.frame(day = state$day, r.vals = state$r.vals)   # Note can add new data here if desired
  df1 &lt;- df %&gt;%
    add_ci(gam.glm, names = c(&quot;lwr&quot;,&quot;upr&quot;)) %&gt;%
    add_pi(gam.glm, names = c(&quot;plwr&quot;,&quot;pupr&quot;))%&gt;%
    mutate(type = &quot;parametric&quot;)
  output[j,]$ucb=df1[nrow(df1),]$upr
  output[j,]$curr.r&lt;-df1[nrow(df1),]$pred
  output[j,]$curr.emp.r&lt;-df1[nrow(df1),]$r.vals


  output&lt;-output%&gt;%mutate(current.rt=exp(b0+b1*max(rt_est$day)))

  new.cases[j,]&lt;-state$daily_new_cases


}



#######################################################

#Create generating distribution
mGT.params&lt;-weibullpar(4.8, 2.3, loc = 0)
alpha&lt;-mGT.params[2] # called shape in weibullpar, alpha in a discrete Weilbull
beta&lt;-mGT.params[1] # called scale in weibullpar, beta in a discrete Weibull.
q&lt;-exp(-as.numeric(alpha)^(-as.numeric(beta)))
#Discretize Weibull via the extraDistr package&#39;s ddweibull function
w&lt;- ddweibull(0:10, as.numeric(q), as.numeric(beta))




#March 22nd start corresponds to day 60 from dataset
us.df&lt;-data.frame(day=NA,pred=NA,state=NA)
collection.of.states&lt;-unique(output$state)

for(i in 1:length(collection.of.states)){
  state.df&lt;-data.frame(day=seq(lubridate::today(),lubridate::today()+14,by=&#39;days&#39;),pred=NA,state=NA)
  b0&lt;-output[i,]$b0 
  b1&lt;-output[i,]$b1
  t&lt;-seq(5,67,1) #skip forward mean of Weibull on average as
  #R(t) is what GOT us here, not where we&#39;re going
  
  r.vals&lt;-exp(b0+b1*t) #R val function
  cases&lt;-new.cases[i,]
  pred.vals&lt;-matrix(0,nrow=100,ncol=1)
  place.hold&lt;-pred.vals
  
  for(j in 1:(length(cases)+14)){
    if(j&lt;= length(cases)){ #If we have data, use data
      place.hold&lt;-matrix(0,nrow=100,ncol=1)
      place.hold[(j+1):(j+11)]&lt;-r.vals[j]*cases[j]*w
      pred.vals&lt;-pred.vals+place.hold
    }else{ #Else use projections
      place.hold&lt;-matrix(0,nrow=100,ncol=1)
      place.hold[(j+1):(j+11)]&lt;-r.vals[j]*pred.vals[j]*w
      pred.vals&lt;-pred.vals+place.hold
    }
  }
  state.df$pred&lt;-pred.vals[(length(cases)):(length(cases)+14)]
  state.df$state&lt;-collection.of.states[i]
  us.df&lt;-rbind(us.df,state.df)
  us.df&lt;-na.omit(us.df)
}</code></pre>
</div>
