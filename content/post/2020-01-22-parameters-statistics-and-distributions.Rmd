---
title: Parameters, Statistics, and Distributions
author: Nicholas J. Clark
date: '2020-01-22'
slug: parameters-statistics-and-distributions
categories:
  - Definitions
tags:
  - Class
description: ''
---

It's easy in the study of statistics to get lost in the definitions.  Oftentimes we throw around words without ever really thinking about what they mean.  Two of the terms that we will quite a bit in this course are parameters and statistics.

## Parameter

I like to think of a parameter as an unobserved value that fully names a distribution.  To put a bit of meat on this definition let's consider our data coming from a Normal distribution.  The probability density function for a Normal distribution is:

$$ f(y_i)=\frac{1}{\sqrt{2\pi}\sigma} e^{\frac{1}{2\sigma^2}(y-\mu)^2}$$

Here there are two parameters, $\mu$ and $\sigma$.  These parameters control the center and the scale of our bell shaped curve.

How does this relate to our statistical modeling approach we've been doing up to this point?  Well, if we assume

$$y_i=\mu+\epsilon_i$$
and $\epsilon_i \sim N(0,\sigma)$, then our data follows the PDF above.

Therefore there are two parameters in this model that we would need to estimate from our data, $\mu$ and $\sigma$.  We can think of our data, $y_i$ as having been generated by this distribution.

If we extend this to our two means model our data then have PDF

$$f(y_{i,j})=\frac{1}{\sqrt{2\pi}\sigma} e^{\frac{1}{2\sigma^2}(y-\mu_i)^2}$$

Recall we can also write this as:

$$f(y_{i})=\frac{1}{\sqrt{2\pi}\sigma} e^{\frac{1}{2\sigma^2}(y-\beta_0-\beta_1 x_i)^2}$$


Where $x_i=1$ if in group 1, else $x_i=0$.

Here we see that our data generating mechanism has three parameters to be estimated, $\beta_0$, $\beta_1$, $\sigma$.  If we know those three parameters we know everything we could possibly know about the distribution of $y_i$

## Statistics

So if these are parameters, what are Statistics?  A statistic is virtually any function of our data.  For instance, $g(\boldsymbol{y})=\max(\boldsymbol{y})$ is a statistic, or $h(\boldsymbol{y})=\bar{y}$ is a statistic.

So if ANYTHING is a statistic, how does this help us?  Well, in general what we want to calculate are statistics that help us either answer a question of interest or to estimate a parameter.

For instance, in our single means model we might want to use $\bar{y}$ as a statistic because it helps us estimate $\mu$.  However, this doesn't help us estimate $\sigma$ so perhaps we would use $S^2=\frac{\sum_i (y_i-\bar{y})^2}{n-1}$ as another statistic.

In the multiple means model we have three parameters to estimate so we may need three statistics to help us here.  So this is helpful, but we also want to answer questions like, is our model better than the null model.  Here we will have multiple statistics that help address the question of interest.  For instance we have $R^2$ as a statistic.  Note that $R^2$ is a function of our data so it is a statistic.  

However, this brings us to another important note on what statistic we might want to calculate.  It will be extremely beneficial to use statistics that have \textit{known distributions}.  For instance, if $\epsilon_i \sim N(0,\sigma)$, then we know the PDF of $y_i$ and in turn we know that $\frac{\bar{y}-\mu}{\frac{s}{\sqrt{n}}}\sim t_{n-1}$.  So $R^2$ would be a good statistic if we \textit{knew} the distribution of it.  However, we unfortunately don't have a distribution in general for $R^2$.  One way around this is to simulate a distribution for $R^2$. 

Here's an example using the scent data from class

Here's our $R^2$
```{r,warning=FALSE,message=FALSE}
library(tidyverse)
scent.dat<-read.table("http://www.isi-stats.com/isi2/data/OdorRatings.txt",header=TRUE)
scent.mod<-lm(rating~condition,data=scent.dat)
our.stat<-summary(scent.mod)$r.squared
```

If scent doesn't matter then we could shuffle our conditions around 


```{r}
M <- 1000
emp.est<-data.frame(shuffle=seq(1,M),stat=NA)#Blank data set to fill in
for(i in 1:M){
  scent.dat.rearr = scent.dat %>%
    mutate(new_cond=sample(scent.dat$condition))
  shuff.scent.mod<-lm(rating~new_cond,data=scent.dat.rearr)
  emp.est[i,]$stat<-summary(shuff.scent.mod)$r.squared
}
```

So even though we don't know the distribution of $R^2$ we know it sort of looks like:

```{r}
emp.est %>% ggplot(aes(x=stat))+geom_histogram(bins=20)
```

If we knew the distribution of $R^2$ then we could find the area of the curve to the right of OUR statistic to find how rare it would be to observe something as extreme or more extreme then what we observed.  Here we can just count:

```{r}
emp.est %>% filter(stat > our.stat)%>%
  summarize(p.val=n()/nrow(emp.est))
```


Here we see that it is very rare to observe something as extreme as our statistic, so we might conclude that there is a relationship between scent and score.